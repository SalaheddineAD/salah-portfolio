const pdetails = [
  {
    id: 1,
    title: "ML-driven Bank's Customer Segmentation",
    role: "Data Scientist Associate at AkumenIA",
    Description:[
      "As a Data Scientist Associate at AkumenIA, I worked on an AI-driven customer segmentation project to enhance the bank's customer segmentation strategy and drive business growth. The project involved several steps, starting with data quality reporting, where we prepared a data dictionary and analyzed the bank's customer data to identify and report null values, incoherent values, outliers, invalid data, and duplicates.",
      "Next, we created scoring variables to assess the quality of customer segmentation based on the bank's objectives and goals. We then developed over 100 Key Performance Indicators (KPIs) for use in machine learning models, cleaned and structured the data, and employed machine learning algorithms such as hierarchical clustering and K-means for customer segmentation.",
      "By iteratively refining the models and selecting the best-performing model based on scoring variables, we were able to achieve over 96% accuracy in segment assignment without using the most important KPI, and 99.99% accuracy when including the key KPI. The project also involved sharing insights on the characteristics of each customer segment and providing detailed customer profiling to inform the bank's marketing and sales strategies.",
      "Ultimately, our team's approach significantly improved the bank's customer segmentation strategy, empowering the bank to make more informed decisions and tailor its marketing and sales strategies to each segment, resulting in business growth. The successful development of a highly accurate assignment engine also enabled the bank to accurately determine the segment to which a client belongs, further improving the overall customer experience"
    ]
  },
  {
    id: 2,
    title: "Investment Intentions Forecasting",
    role: "Data Scientist Associate at AkumenIA",
    Description:[
      "As the project lead for a team of two Data Scientists, I worked on a comprehensive fraud detection system for Atlanta Insurance's automobile claims. The objective was to develop a robust system that could effectively detect fraudulent claims using machine learning techniques.",
      "Our team utilized a meticulous methodology that began with data preparation and feature engineering, which involved data cleaning, duplicate detection, correction of incoherent values, and feature engineering in the insurance claims dataset.",
      "We also addressed the issue of unbalanced data by using a semi-supervised methodology to ensure the models were trained effectively on both fraudulent and non-fraudulent cases. Our team created supervised machine learning models, including AdaBoost, XGBoost, and Support Vector Machine (SVM), to detect fraudulent claims while minimizing false positive cases. Additionally, we utilized unsupervised learning techniques such as Isolation Forest and Density-Based Spatial Clustering of Applications with Noise (DBSCAN) to detect potential fraud as outliers in the dataset.",
      "To implement the project, we used Python, scikit-learn, CUDA, PySpark, Power BI, and XGBoost technologies. Our team's approach successfully developed a comprehensive fraud detection system that effectively identified fraudulent cases while minimizing false positives. This allowed Atlannta Insurance to streamline their claims processing and mitigate the financial impact of fraudulent claims.",
      "As the project lead, I was responsible for ensuring that the project adhered to best practices, timelines, and quality standards. My contribution to the project included driving the team's methodology, supervising feature engineering and model development, as well as ensuring that the project was completed within schedule and budget. Working as a team of two data scientists, we were able to deliver a project that met the companyâ€™s needs and exceeded its expectations."
    ]
  },
  {
    id: 3,
    title: "Energy Production, Consumption and Demand Forecasting",
    role: "Data Scientist Associate at AkumenIA",
    Description:[
      "As a Data Scientist Associate at AkumenIA, I worked on a project to create highly accurate forecasting models for energy production, consumption, and demand. The project involved a meticulous methodology that began with data quality reporting, where we analyzed the dataset to identify missing data, duplicates, incoherent data, and outliers. We prepared a report on the analysis for the client, which helped them to better understand the quality of their data.",
      "We then cleaned and structured the dataset to be suitable for machine learning models. In the first iteration of building machine learning models, we utilized the client's data to create forecasting models for energy production, demand, and consumption. We achieved Mean Absolute Percentage Errors (MAPE) of 0.015%, 0.03%, and 0.035% for energy production, demand, and consumption models, respectively, using models like Long Short-Term Memory (LSTM), Temporal Fusion Transformer (TFT), and Facebook's Prophet.",
      "The client desired to further improve model accuracy, so we scraped additional data that might have correlations with target variables, such as Google Trends, Human Capital Project (HCP) indices, Bank Al-Maghrib (BKAM) data, press releases, and weather data. In the second iteration of building machine learning models, we reiterated the forecasting models using external data to improve accuracy. We achieved MAPE values of 0.009%, 0.023%, and 0.028% for energy production, demand, and consumption models, respectively.",
      "As a team of Data Scientist Associates at AkumenIA, we successfully created highly accurate forecasting models for energy production, consumption, and demand, meeting the client's goal."
    ]
  },
  {
    id: 4,
    title: "Urban Mobility Analysis",
    role: "Data Scientist Associate at Atlanta Insurance",
    Description:[
      "I worked on a challenging project involving the analysis of urban mobility using GPS data. The objective of the project was to identify bottlenecks, create mobility flow datasets, and visualize the results using various tools. Our team faced several challenges in this project, including working with GPS data from multiple sources and formats, and the need to use advanced clustering algorithms to identify urban bottlenecks.",
      'We began the project by collecting and exploring GPS data from various sources such as smartphones, IoT devices, and transportation systems. The data preprocessing step was particularly important, as we needed to eliminate noise, inaccuracies, and missing values to ensure that the data was suitable for further analysis. We also used DBSCAN, a powerful clustering algorithm, to identify urban bottlenecks and high-density areas with traffic congestion or limited mobility.',
      "Once we had created a mobility flow dataset using the processed GPS data and DBSCAN results, we analyzed the dataset to identify patterns, trends, and correlations that could impact urban mobility and traffic management. Our team utilized a variety of tools, such as Power BI, Folium, and Plotly, to visualize the results of the urban mobility analysis. By plotting graphs and heatmaps, we were able to display traffic congestion, bottlenecks, mobility patterns, and other relevant insights.",
      "The visualizations and statistics we produced provided valuable insights for stakeholders and decision-makers, empowering them to make data-driven urban planning and traffic management decisions."
    ]
  },
  {
    id: 5,
    title: "Cashflow Forecasting",
    role: "Data Scientist Associate at AkumenIA",
    Description:[
      
    ]
  },
  {
    id: 6,
    title: "Energy Production Forecasting",
    role: "Data Scientist Associate at AkumenIA",
    Description:[
      
    ]
  },
  {
    id: 7,
    title: "Fraudulent Automobiile Claims Detection",
    role: "Data Scientist at Atlanta Insurance",
    Description:[
      "As the project lead for a team of two Data Scientists, I worked on a comprehensive fraud detection system for Atlanta Insurance's automobile claims. The objective was to develop a robust system that could effectively detect fraudulent claims using machine learning techniques.",
      "Our team utilized a meticulous methodology that began with data preparation and feature engineering, which involved data cleaning, duplicate detection, correction of incoherent values, and feature engineering in the insurance claims dataset.",
      "We also addressed the issue of unbalanced data by using a semi-supervised methodology to ensure the models were trained effectively on both fraudulent and non-fraudulent cases. Our team created supervised machine learning models, including AdaBoost, XGBoost, and Support Vector Machine (SVM), to detect fraudulent claims while minimizing false positive cases. Additionally, we utilized unsupervised learning techniques such as Isolation Forest and Density-Based Spatial Clustering of Applications with Noise (DBSCAN) to detect potential fraud as outliers in the dataset.",
      "To implement the project, we used Python, scikit-learn, CUDA, PySpark, Power BI, and XGBoost technologies. Our team's approach successfully developed a comprehensive fraud detection system that effectively identified fraudulent cases while minimizing false positives. This allowed Atlanta Insurance to streamline its claims processing and mitigate the financial impact of fraudulent claims.",
      "As the project lead, I was responsible for ensuring that the project adhered to best practices, timelines, and quality standards. My contribution to the project included driving the team's methodology, supervising feature engineering and model development, as well as ensuring that the project was completed within schedule and budget. Working as a team of two data scientists, we were able to deliver a project that met the companyâ€™s needs and exceeded its expectations."
    ]
  },
  {
    id: 8,
    title: "Student Academic Success Forecasting",
    role: "Data Scientist Intern at National School of Applied Sciences of Khouribga, Morocco",
    Description:[
      "I took on the challenging project of predicting which students were likely to face learning difficulties and developing a targeted support system to help them succeed academically. The project involved several complex steps, starting with data cleaning, analysis, and visualization. I ensured the quality and integrity of the educational dataset, conducted a comprehensive analysis of the data, and visualized the data using tools such as Seaborn and Pandas to generate insights to inform the development of the predictive model.",
      "I proceeded with feature engineering and selection, creating and selecting relevant features from the dataset such as demographic information, academic history, and social factors to be used as input for the machine learning model. Using scikit-learn, I developed a supervised machine learning model to predict students who may face learning difficulties and conducted model evaluation using cross-validation, performance metrics, and comparison to alternative models to ensure the chosen model achieved the highest accuracy and generalizability.",
      "The project was implemented using Python, scikit-learn, Pandas, and Seaborn. The successful development and deployment of the predictive model allowed educational institutions to proactively identify students who may face learning difficulties. By providing targeted support and interventions, educators could help these students overcome challenges and achieve their full academic potential."
      
    ]
  },
  {
    id: 9,
    title: "Drowsiness Detection",
    role: "Personal Project",
    Description:[
      
    ]
  }
]

// const pdetails = [
//   {
//     id: 1,
//     title: "AI-driven Customer Segmentation for a Bank",
//     role: "Data Scientist Associate at AkumenIA",
//     objective: "To enhance the bank's customer segmentation strategy using AI and machine learning techniques to drive business growth.",
//     methodologies: [
//       {
//         title: "Data Quality Reporting:",
//         details: [
//           "Prepared a data dictionary and analyzed the bank's customer data.",
//           "Identified and reported null values, incoherent values, outliers, invalid data, and duplicates."
//         ]
//       },
//       {
//         title: "Scoring Variables Creation:",
//         details: [
//           "Developed scoring variables to assess the quality of customer segmentation based on the bank's objectives and goals."
//         ]
//       },
//     ],
//     result: "By leveraging AI and machine learning techniques, we significantly improved the bank's customer segmentation strategy. Our approach not only provided insights into the characteristics and profiles of each customer segment but also led to the development of a highly accurate assignment engine. This project's success empowered the bank to make more informed decisions, tailor its marketing and sales strategies to each segment, and ultimately drive business growth."
//   },
//   {
//     id: 2,
//     title: "Title of Popup 2",
//     overview: "",
//     methodology: "",
//     result: ""
//   },
//   {
//     id: 3,
//     title: "Title of Popup 3",
//     overview: "",
//     methodology: "",
//     result: ""
//   },
//   {
//     id: 4,
//     title: "Title of Popup 4",
//     overview: "",
//     methodology: "",
//     result: ""
//   },
// ]